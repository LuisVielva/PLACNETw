#!/bin/bash

# Start to count time
date0=$(date +"%s")

# Check there is no other PLACNETw task in progress
numdirs=$(ls -l /media/luis/ADATA/progress | grep ^d | wc -l)
numfiles=$(ls -l /media/luis/ADATA/progress | grep ^- | wc -l)
if [[ $numdirs != 0 || $numfiles != 0 ]]; then 
    #echo "Work already in progress"
    exit 1
fi

# Check that arguments have been provided to the script and that the argument is a directory
if [[ $# -eq 0 || ! -d "$1" ]]; then
    #echo "Bad arguments"
    exit 1
fi

# Check there are exactly three files in the directory
files=($1/*)
numfiles=${#files[@]}
if [[ $numfiles != 3 ]]; then
    #echo "Bad number of files"
    exit 1
fi

# Check that the those files are the expected ones
unique=$(echo $1 | awk 'BEGIN { FS = "/" } ; { print $NF }')
if [[ ! -f "$1/$unique.req" || ! -f "$1/R_1.fastq.gz" || ! -f "$1/R_2.fastq.gz" ]]; then
    #echo "Bad file names"
    exit 1
fi

# Read the fields from the request file
read unique email name <<< $(< $1/$unique.req)

# Uncompress the first read file
gzip -f -d $1/R_1.fastq.gz
# Check R_1.fastq has @ as the first character of the first line, A|T|C|G|N as the 1st of 2nd line, and + as 1st of 3rd line
l1=$(head -1 $1/R_1.fastq)
l2=$(head -2 $1/R_1.fastq | tail -1)
l3=$(head -3 $1/R_1.fastq | tail -1)
if [ ${l1:0:1} != "@" ] || [[ ! "ATCGN" =~ ${l2:0:1} ]] || [ ${l3:0:1} != "+" ]
then
    /home/luis/bin/senderrormail.py $email $name "Data seems not to be in fastq format"
    /home/luis/bin/senderrormail.py "LuisVielva@gmail.com" $name "Data seems not to be in fastq format"
    rm -rf $1
    exit 1
fi

# Uncompress the second read file
gzip -f -d $1/R_2.fastq.gz
# Check R_2.fastq has @ as the first character of the first line, A|T|C|G|N as the 1st of 2nd line, and + as 1st of 3rd line
l1=$(head -1 $1/R_2.fastq)
l2=$(head -2 $1/R_2.fastq | tail -1)
l3=$(head -3 $1/R_2.fastq | tail -1)
if [ ${l1:0:1} != "@" ] || [[ ! "ATCGN" =~ ${l2:0:1} ]] || [ ${l3:0:1} != "+" ]
then
    /home/luis/bin/senderrormail.py $email $name "Data seems not to be in fastq format"
    /home/luis/bin/senderrormail.py "LuisVielva@gmail.com" $name "Data seems not to be in fastq format"
    rm -rf $1
    exit 1
fi

# Move the folder to the 'progress' directory
mv $1 /media/luis/ADATA/progress

# Change the current working directory
cd "/media/luis/ADATA/progress/$unique"

# Get reads length
READLENGTH="$(cat R_1.fastq | head -2 | tail -1 | wc -c)"
echo "READLENGTH:" $READLENGTH

# Shuffle the sequences
# shuffleSequences_fastq.pl R_1.fastq R_2.fastq R_all.fastq
    
# Perform the assembly
VelvetOptimiser.pl --t 70 --s $(($READLENGTH-32)) --e $READLENGTH --x 4 --d optim -f '-fastq -shortPaired -separate R_1.fastq R_2.fastq'

# Change to the directory with the optimal solution
cd optim    
    
# Get insert length
FLANK="$(grep "Paired-end library 1 has length" *_Logfile.txt | head -1 | awk '{print $6}' | sed 's/,//')"
INSERT=$(($FLANK - 2*$READLENGTH))
echo "Flank, Insert:", $FLANK, $INSERT
     
# Generate index file
bowtie2-build contigs.fa reference.index
echo "bowtie2-build"
    
# Use Bowtie to create mapping.sam
# Discard header lines and those with "="
bowtie2 -1 ../R_1.fastq -2 ../R_2.fastq -x reference.index --no-head -p 70 -a | awk '$7!="=" {print $line}' > mapping.sam
echo "bowtie2"

# Placnet:
# 
blastn -query contigs.fa -db /media/luis/ADATA/BD/all_ncbi.fna -out tmpMegaBlast.txt -num_alignments 0 -evalue 1e-25 -num_threads 70
echo "blastn"

# 
blastRefDB.pl tmpMegaBlast.txt > blastRefDB.csv

#
sam2scaffold.pl mapping.sam contigs.fa $FLANK 200 > sam2scaffold.csv

#
cat blastRefDB.csv sam2scaffold.csv > red.csv
cp red.csv red0.csv

# Prodigal
prodigal -a placnet.prod.faa -d placnet.prod.cds -i contigs.fa  -p meta -o prodigal.log

# 
blastp -query placnet.prod.faa -db /media/luis/ADATA/BD/REL-DB.faa -outfmt 6 -evalue 1e-50 -out tmpREL.blast -num_alignments 1 -num_threads 70
blastp -query placnet.prod.faa -db /media/luis/ADATA/BD/RIP-DB.faa -outfmt 6 -evalue 1e-50 -out tmpRIP.blast -num_alignments 1 -num_threads 70
blastn -query contigs.fa       -db /media/luis/ADATA/BD/INC-DB.fna -outfmt 6 -evalue 1e-50 -out tmpInc.blast -num_alignments 1 -num_threads 70

# 
awk '{split($1, a1, /\_/); print a1[1] "_" a1[2] "_" a1[3] "_" a1[4] "_" a1[5] "_" a1[6] "\t" $2 "\t" $3}' tmpREL.blast > rel.csv
awk '{split($1, a1, /\_/); print a1[1] "_" a1[2] "_" a1[3] "_" a1[4] "_" a1[5] "_" a1[6] "\t" $2 "\t" $3}' tmpRIP.blast > rip.csv
awk '{print $1 "\t" $2 "\t"$3}' tmpInc.blast > inc.csv

# Split contigs.fa in a file per node
fastasplit.py

# Generate HTML with additional information and reference.csv file from 
# blastRefDB.csv that contains lines with the following pattern:
# NODE_1_length_6166_cov_38.304737	hit	gi|690030267|ref|NC_025100.1|	11531	0.0
#
# For each HTML, we execute a command like
# contigInReferenceInfo NODE_1 NC_022648
# 
# and for reference.csv we execute head -1 ~/fna/$ref.fna | awk '{for (i=2; i<NF; i++) printf $i " "; print $NF}'
rm -f reference.csv
while read in
do
    node=$(echo "$in" | awk '{split($1, a1, /\_/); print a1[1] "_" a1[2];}')
    ref=$(echo "$in" | awk '{split($3, a3, /\|/); print a3[4];}' | awk '{split($1, a1, /\./); print a1[1];}' )
    hdr=$(head -1 /media/luis/ADATA/fna/$ref.fna | awk '{for (i=2; i<NF; i++) printf $i " "; print $NF}')
    echo -e "$ref\t$hdr" >> reference.csv
    cmd="/home/luis/bin/contigInReferenceInfo $node $ref"
    $cmd &
done < blastRefDB.csv
wait

# Build the data:

# Total time
date1=$(date +"%s"); diff=$(($date1-$date0)); echo "$(($diff / 60)):$(($diff % 60))" > go.log

# Copy all the data to the www directory
cp ../*.req .
directorio="/media/luis/ADATA/uploads/$unique"
mkdir $directorio
chmod 777 $directorio
mv NODE_* reference.csv red0.csv ???.csv *.req go.log placnet.prod.* tmp*.blast *Logfile.txt $directorio
chmod 777 $directorio/*

# Send an email to the administrator (optional) and another one to the user
sendmail.py "LuisVielva@gmail.com" $name $unique
sendmail.py $email $name $unique

# Clean up
cd /media/luis/ADATA
rm -rf /media/luis/ADATA/progress/$unique
